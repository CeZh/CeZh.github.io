<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ce Zhang   | publications</title>
    <meta name="author" content="Ce Zhang  " />
    <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar." />
    <meta name="keywords" content="Autonomous Vehicle, Machine Learning/Deep Learning, Robotics" />


    <!-- Bootstrap & MDB -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://cezh.github.io/publications/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://CeZh.github.io/"><span class="font-weight-bold">Ce Zhang</span>   </a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2019</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="doi:10.1504/IJVS.2019.101857" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A simulation-based comparative study on lateral characteristics of trucks with double and triple trailers</div>
          <!-- Author -->
          <div class="author">Chen, Yang,Â Peterson, Andrew W.,Â 
                  <em>Zhang, Ce</em>,Â and Ahmadian, Mehdi
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>International Journal of Vehicle Safety</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://www.inderscienceonline.com/doi/pdf/10.1504/IJVS.2019.101857" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p> This paper investigates the lateral stability and manoeuvrability in long combination vehicles (LCVs), namely semi-trucks with 28-ft doubles, 28-ft triples, and 33-ft doubles, using TruckSim. In particular, the likelihood of rollovers, rearward amplification, and off-tracking are analysed among those LCVs using the multi-domain dynamic models developed in TruckSim. The efforts to validate the truck dynamic model against test results are also included. The simulation results show that trucks with triple trailers exhibit a larger rearward amplification, higher likelihood of rollovers, and larger off-tracking than trucks with double trailers. Additionally, the results indicate that increasing the trailer length from 28 to 33 feet does not increase the likelihood of rollovers or the rearward amplification. In fact, the longer trailers provide a slight amount of additional roll stability due to their longer wheelbase. </p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IEEE-EMBC</abbr></div>

        <!-- Entry bib key -->
        <div id="9176705" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Computationally Efficient Multiclass Time-Frequency Common Spatial Pattern Analysis on EEG Motor Imagery</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Ce</em>,Â and Eskandarian, Azim
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2020 42nd Annual International Conference of the IEEE Engineering in Medicine Biology Society (EMBC)</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2008.11227" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://ieeexplore.ieee.org/document/9176705" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Common spatial pattern (CSP) is a popular feature extraction method for electroencephalogram (EEG) motor imagery (MI). This study modifies the conventional CSP algorithm to improve the multi-class MI classification accuracy and ensure the computation process is efficient. The EEG MI data is gathered from the Brain-Computer Interface (BCI) Competition IV. At first, a bandpass filter and a time-frequency analysis are performed for each experiment trial. Then, the optimal EEG signals for every experiment trials are selected based on the signal energy for CSP feature extraction. In the end, the extracted features are classified by three classifiers, linear discriminant analysis (LDA), naÃ¯ve Bayes (NVB), and support vector machine (SVM), in parallel for classification accuracy comparison. The experiment results show the proposed algorithm average computation time is 37.22% less than the FBCSP (1 st winner in the BCI Competition IV) and 4.98% longer than the conventional CSP method. For the classification rate, the proposed algorithm kappa value achieved 2nd highest compared with the top 3 winners in BCI Competition IV.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IOP-JNE</abbr></div>

        <!-- Entry bib key -->
        <div id="Zhang_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">EEG-inception: an accurate and robust end-to-end neural network for EEG-based motor imagery classification</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Ce</em>,Â Kim, Young-Keun,Â and Eskandarian, Azim
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Journal of Neural Engineering</em> Mar 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2101.10932" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://iopscience.iop.org/article/10.1088/1741-2552/abed81" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Objective. Classification of electroencephalography (EEG)-based motor imagery (MI) is a crucial non-invasive application in brainâ€“computer interface (BCI) research. This paper proposes a novel convolutional neural network (CNN) architecture for accurate and robust EEG-based MI classification that outperforms the state-of-the-art methods. Approach. The proposed CNN model, namely EEG-inception, is built on the backbone of the inception-time network, which has showed to be highly efficient and accurate for time-series classification. Also, the proposed network is an end-to-end classification, as it takes the raw EEG signals as the input and does not require complex EEG signal-preprocessing. Furthermore, this paper proposes a novel data augmentation method for EEG signals to enhance the accuracy, at least by 3%, and reduce overfitting with limited BCI datasets. Main results. The proposed model outperforms all state-of-the-art methods by achieving the average accuracy of 88.4% and 88.6% on the 2008 BCI Competition IV 2a (four-classes) and 2b datasets (binary-classes), respectively. Furthermore, it takes less than 0.025 s to test a sample suitable for real-time processing. Moreover, the classification standard deviation for nine different subjects achieves the lowest value of 5.5 for the 2b dataset and 7.1 for the 2a dataset, which validates that the proposed method is highly robust. Significance. From the experiment results, it can be inferred that the EEG-inception network exhibits a strong potential as a subject-independent classifier for EEG-based MI tasks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IEEE/CAA-JAS</abbr></div>

        <!-- Entry bib key -->
        <div id="9242336" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Survey and Tutorial of EEG-Based Brain Monitoring for Driver State Analysis</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Ce</em>,Â and Eskandarian, Azim
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CAA Journal of Automatica Sinica</em> Mar 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2008.11226" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://ieeexplore.ieee.org/document/9242336" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The driverâ€™s cognitive and physiological states affect his/her ability to control the vehicle. Thus, these driver states are essential to the safety of automobiles. The design of advanced driver assistance systems (ADAS) or autonomous vehicles will depend on their ability to interact effectively with the driver. A deeper understanding of the driver state is, therefore, paramount. Electroencephalography (EEG) is proven to be one of the most effective methods for driver state monitoring and human error detection. This paper discusses EEG-based driver state detection systems and their corresponding analysis algorithms over the last three decades. First, the commonly used EEG system setup for driver state studies is introduced. Then, the EEG signal preprocessing, feature extraction, and classification algorithms for driver state detection are reviewed. Finally, EEG-based driver state monitoring research is reviewed in-depth, and its future development is discussed. It is concluded that the current EEG-based driver state monitoring algorithms are promising for safety applications. However, many improvements are still required in EEG artifact reduction, real-time processing, and between-subject classification accuracy.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IEEE-ITSC</abbr></div>

        <!-- Entry bib key -->
        <div id="9564709" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Attention-based Neural Network for Driving Environment Complexity Perception (Best Student Paper Award)</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Ce</em>,Â Eskandarian, Azim,Â and Du, Xuelai
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC)</em> Mar 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2106.11277" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://ieeexplore.ieee.org/document/9242336" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The driverâ€™s cognitive and physiological states affect his/her ability to control the vehicle. Thus, these driver states are essential to the safety of automobiles. The design of advanced driver assistance systems (ADAS) or autonomous vehicles will depend on their ability to interact effectively with the driver. A deeper understanding of the driver state is, therefore, paramount. Electroencephalography (EEG) is proven to be one of the most effective methods for driver state monitoring and human error detection. This paper discusses EEG-based driver state detection systems and their corresponding analysis algorithms over the last three decades. First, the commonly used EEG system setup for driver state studies is introduced. Then, the EEG signal preprocessing, feature extraction, and classification algorithms for driver state detection are reviewed. Finally, EEG-based driver state monitoring research is reviewed in-depth, and its future development is discussed. It is concluded that the current EEG-based driver state monitoring algorithms are promising for safety applications. However, many improvements are still required in EEG artifact reduction, real-time processing, and between-subject classification accuracy.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="09-09-01-0002" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Brain Wave-Verified Driver Alert System for Vehicle Collision Avoidance</div>
          <!-- Author -->
          <div class="author">Riyahi, Pouria,Â Eskandarian, Azim,Â and <em>Zhang, Ce</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>SAE International Journal of Transportation Safety</em> Apr 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.4271/09-09-01-0002" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Collision alert and avoidance systems (CAS) could help to minimize driver errors. They are instrumental as an advanced driver-assistance system (ADAS) when the vehicle is facing potential hazards. Developing effective ADAS/CAS, which provides alerts to the driver, requires a fundamental understanding of human sensory perception and response capabilities. This research explores the premise that external stimulation can effectively improve driversâ€™ reaction and response capabilities. Therefore this article proposes a light-emitting diode (LED)-based driver warning system to prevent potential collisions while evaluating novel signal processing algorithms to explore the correlation between driver brain signals and external visual stimulation. When the vehicle approaches emerging obstacles or potential hazards, an LED light box flashes to warn the driver through visual stimulation to avoid the collision through braking. Thirty (30) subjects completed a driving simulator experiment under different near-collision scenarios. The Steady-State Visually Evoked Potentials (SSVEP) of the driversâ€™ brain signals and their collision mitigation (control performance) data were analyzed to evaluate the LED warning systemâ€™s effectiveness. The results show that (1) The proposed modified canonical correlation analysis evaluation (CCA-EVA) algorithm can detect SSVEP responses with 4.68% higher accuracy than the Adaptive Kalman filter; (2) The proposed driver monitoring and alert system produce on average a 52% improvement in time to collision (TTC), 54% improvement in reaction distance (RD), and an overall 26% reduction in collision rate as compared to similar tests without the LED warning.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ASME-IMECE</abbr></div>

        <!-- Entry bib key -->
        <div id="10.1115/IMECE2021-69975" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Comparative Analysis of Object Detection Algorithms in Naturalistic Driving Videos</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Ce</em>,Â and Eskandarian, Azim
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ASME International Mechanical Engineering Congress and Exposition</em> Nov 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1115/IMECE2021-69975" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Intelligent vehicle research has been rapidly developing. Object detection is one of the most critical study areas for intelligent vehicleâ€™s driving safety. This paper conducts a comparative analysis for two popular real-time object detection algorithms: Yolo-v4 and deconvolutional single shot multibox detector (DSSD) under a naturalistic driving environment. An 80-classes COCO dataset trains each neural network at first, then fine-tuned by the BDD100k dataset. The detection results are compared by True/False Positive Results, Precision-Recall Curve, and average precision @ intersection of union 50 and average precision @ intersection of union 75 results. According to the analysis results, the Yolo-v4 outperforms the DSSD algorithm in bad weather, nighttime conditions, and small object detections. The Yolo-v4 and DSSD mean average precision for the BDD dataset is 22.63 and 11.86, respectively. The Yolo-v4 precision is 90.81\% better than the DSSD algorithm, proving that the Yolo-v4 is a better fit for real-world driving environment studies.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="Zhang2022AQI" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Quality Index Metric and Method for Online Self-Assessment of Autonomous Vehicles Sensory Perception</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Ce</em>,Â and Eskandarian, Azim
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>ArXiv</em> Nov 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Perception is critical to autonomous driving safety. Camera-based object detection is one of the most important methods for autonomous vehicle perception. Current camerabased object detection solutions for autonomous driving cannot provide feedback on the detection performance for each frame. We propose an evaluation metric, namely the perception quality index (PQI), to assess the camera-based object detection algorithm performance and provide the perception quality feedback frame by frame. The method of the PQI generation is by combining the fine-grained saliency map intensity with the object detection algorithmâ€™s output results. Furthermore, we developed a superpixel-based attention network (SPA-NET) to predict the proposed PQI evaluation metric by using raw image pixels and superpixels as input. The proposed evaluation metric and prediction network are tested on three open-source datasets . The proposed evaluation metric can correctly assess the camera-based perception quality under the autonomous driving environment according to the experiment results. The network regression Rsquare values determine the comparison among models. It is shown that a Perception Quality Index is useful in self-evaluating a cameraâ€™s visual scene perception</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2022 Ce Zhang  . Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>
  </body>

  <!-- jQuery -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  <!-- Mansory & imagesLoaded -->
  <script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
  <script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
  
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  
</html>

